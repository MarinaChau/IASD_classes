{"cells":[{"cell_type":"markdown","id":"c925ef84","metadata":{"id":"c925ef84"},"source":["# K-bandits"]},{"cell_type":"code","execution_count":1,"id":"b4897d8c","metadata":{"id":"b4897d8c","executionInfo":{"status":"ok","timestamp":1643234810929,"user_tz":-60,"elapsed":20,"user":{"displayName":"Marina Chau","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07743346366730199581"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"id":"231e3ca3","metadata":{"id":"231e3ca3","executionInfo":{"status":"ok","timestamp":1643234810931,"user_tz":-60,"elapsed":19,"user":{"displayName":"Marina Chau","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07743346366730199581"}}},"outputs":[],"source":["n_machines = 10\n","n_tokens = 1000\n","n_runs = 2000"]},{"cell_type":"code","execution_count":3,"id":"b175ddbf","metadata":{"id":"b175ddbf","executionInfo":{"status":"ok","timestamp":1643234810932,"user_tz":-60,"elapsed":17,"user":{"displayName":"Marina Chau","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07743346366730199581"}}},"outputs":[],"source":["def argmax(a):\n","    return max(range(len(a)), key=lambda x: a[x])"]},{"cell_type":"code","execution_count":4,"id":"4bc3c065","metadata":{"id":"4bc3c065","executionInfo":{"status":"ok","timestamp":1643234811484,"user_tz":-60,"elapsed":568,"user":{"displayName":"Marina Chau","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07743346366730199581"}}},"outputs":[],"source":["class Agent():\n","    \n","    def __init__(self, n_machines=10, algorithm=\"greedy_epsilon\", epsilon=0, alpha=None, c_ucb=2, init=None):\n","        \n","        \n","        self.n_machines = n_machines # Total number of machines\n","        self.t = 1 # Step count\n","        self.N = np.zeros(n_machines) # Step count for each machine\n","        self.alpha = alpha # Learning rate\n","        self.algorithm = algorithm\n","        self.actions = np.arange(n_machines)\n","        \n","        self.mean_reward = 0\n","        #self.reward = np.zeros()\n","        \n","        # For epsilon greedy policy\n","        self.epsilon = epsilon\n","        \n","        # For UCB criterion\n","        self.c = c_ucb\n","        \n","        # For gradient bandit\n","        self.H = np.zeros(n_machines)\n","        \n","        # Initialisation of the Q-value function\n","        if init: # Optimistic initialisation\n","            self.Q = init * np.ones(n_machines)\n","        else:\n","            self.Q = np.random.normal(loc=0., scale=1., size=n_machines)\n","    \n","    def softmax(self):\n","        self.prob_action = np.exp(self.H - np.max(self.H)) / np.sum(np.exp(self.H - np.max(self.H)), axis=0)\n","        \n","    def learn(self, action, reward):\n","        self.t += 1\n","        self.N[action] += 1\n","        if self.alpha:\n","            if self.algorithm == \"gradient\":\n","                self.mean_reward = self.mean_reward + (reward - self.mean_reward) / self.t\n","                self.H[action] = self.H[action] + self.alpha * (reward - self.mean_reward) * (1 -\n","                self.prob_action[action])\n","                actions_not_taken = self.actions != action\n","                self.H[actions_not_taken] = self.H[actions_not_taken] - self.alpha * (reward - self.mean_reward) * self.prob_action[actions_not_taken]\n","            else:\n","                self.Q[action] = self.Q[action] + self.alpha * (reward - self.Q[action])\n","        else:\n","            self.Q[action] = self.Q[action] + (reward - self.Q[action]) / self.N[action]\n","    \n","    def act(self, algorithm):\n","        \n","         # Select action according to epsilon greedy policy\n","        if self.algorithm == \"epsilon_greedy\":\n","            prob = np.random.random()\n","            if prob <= self.epsilon:\n","                action = np.random.choice(n_machines)\n","            else:\n","                action = argmax(self.Q)\n","        \n","        # Select action according to UCB criteria\n","        if self.algorithm == \"ucb\":\n","            action = argmax(self.Q + self.c * np.sqrt(np.log(self.t) / self.N))\n","        \n","        # Gradient\n","        if self.algorithm == \"gradient\":\n","            # Update probabilities\n","            self.softmax()\n","            # Select highest preference action\n","            action = np.random.choice(self.actions, p=self.prob_action)\n","            \n","        return action"]},{"cell_type":"code","execution_count":5,"id":"40f7bb40","metadata":{"id":"40f7bb40","executionInfo":{"status":"ok","timestamp":1643234811486,"user_tz":-60,"elapsed":13,"user":{"displayName":"Marina Chau","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07743346366730199581"}}},"outputs":[],"source":["def simulation(n_machines, n_tokens, n_runs, algorithm, epsilon=None, init=None, alpha=None):\n","    \n","    rewards = np.zeros(n_tokens)\n","    cpt_actions = np.zeros(n_tokens)\n","    \n","    # For every run...\n","    for i_run in tqdm(range(n_runs)):\n","        \n","        # Initialize q_star \n","        q_star = np.random.normal(loc=0., scale=1., size=n_machines) # q* is constant\n","        optimal_action = argmax(q_star)\n","        \n","        # Initialize the agent\n","        agent = Agent(algorithm=algorithm, epsilon=epsilon, alpha=alpha, n_machines=n_machines, init=init)\n","        \n","        # For every token...\n","        for t in range(n_tokens):\n","            \n","            # Choose a machine\n","            action = agent.act(algorithm)\n","            \n","            # Get the reward\n","            reward = np.random.normal(loc=q_star[action], scale=1.) \n","            rewards[t] += reward\n","            \n","            # Update Q-value function\n","            agent.learn(action, reward)\n","            \n","            # Increment the counter if the action chosen is the optimal one\n","            #print(f\"Action {action}, Optimal {optimal_action}\")\n","            if action == optimal_action:\n","                #print(\"+\")\n","                cpt_actions[t] += 1\n","                \n","    # Compute the frequency of choosing the optimal action            \n","    freq_optimal_actions = cpt_actions / n_runs\n","    \n","    return rewards, freq_optimal_actions"]},{"cell_type":"markdown","id":"25664f1a","metadata":{"id":"25664f1a"},"source":["## Epsilon greedy policy"]},{"cell_type":"code","execution_count":6,"id":"10d9cb45","metadata":{"id":"10d9cb45","outputId":"108f9483-a719-41fa-bcfc-2cfa0e0cbf59","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643234843326,"user_tz":-60,"elapsed":31852,"user":{"displayName":"Marina Chau","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07743346366730199581"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [00:32<00:00, 62.04it/s]\n"]}],"source":["rewards_0, freq_optimal_actions_0 = simulation(n_machines, n_tokens, n_runs, epsilon=0, algorithm='epsilon_greedy')\n","rewards_0_avg = rewards_0 / n_runs"]},{"cell_type":"code","execution_count":null,"id":"6c06836e","metadata":{"id":"6c06836e","outputId":"e5ef3c9f-a6bc-4b20-aeee-879891bfb8f9","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":[" 95%|█████████▍| 1895/2000 [00:24<00:01, 74.21it/s]"]}],"source":["rewards_01, freq_optimal_actions_01 = simulation(n_machines, n_tokens, n_runs, epsilon=0.1, algorithm='epsilon_greedy')\n","rewards_01_avg = rewards_01 / n_runs"]},{"cell_type":"code","execution_count":null,"id":"a9b11667","metadata":{"id":"a9b11667"},"outputs":[],"source":["rewards_001, freq_optimal_actions_001 = simulation(n_machines, n_tokens, n_runs, epsilon=0.01, algorithm='epsilon_greedy')\n","rewards_001_avg = rewards_001 / n_runs"]},{"cell_type":"code","execution_count":null,"id":"b6586bc6","metadata":{"id":"b6586bc6"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(rewards_0_avg, label=r'$\\epsilon$ = 0 (purely greedy)')\n","plt.plot(rewards_01_avg, label=r'$\\epsilon$ = 0.1')\n","plt.plot(rewards_001_avg, label=r'$\\epsilon$ = 0.01')\n","plt.title(\"10-armed bandits - average over 2000 runs\\nEpsilon greedy policy\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Average reward\")\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"id":"9d38e5d5","metadata":{"id":"9d38e5d5"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(freq_optimal_actions_0, label=r'$\\epsilon$ = 0 (purely greedy)')\n","plt.plot(freq_optimal_actions_01, label=r'$\\epsilon$ = 0.1')\n","plt.plot(freq_optimal_actions_001, label=r'$\\epsilon$ = 0.01')\n","plt.title(\"10-armed bandits - average over 2000 runs\\nEpsilon greedy policy\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Frequency of choosing the optimal action\")\n","plt.legend()"]},{"cell_type":"markdown","id":"5e54be29","metadata":{"id":"5e54be29"},"source":["## Optimistic initialization"]},{"cell_type":"code","execution_count":null,"id":"3cf6bfa0","metadata":{"id":"3cf6bfa0"},"outputs":[],"source":["rewards_init_5_eps_0, freq_optimal_actions_init_5_eps_0 = simulation(n_machines=n_machines, n_tokens=n_tokens, n_runs=n_runs, epsilon=0, init=5, algorithm='epsilon_greedy', alpha=0.1)\n","rewards_init_5_eps_0_avg = rewards_init_5_eps_0 / n_runs"]},{"cell_type":"code","execution_count":null,"id":"6b1833fa","metadata":{"id":"6b1833fa"},"outputs":[],"source":["rewards_init_0_eps_01, freq_optimal_actions_init_0_eps_01 = simulation(n_machines, n_tokens, n_runs, epsilon=0.1, init=0, algorithm='epsilon_greedy', alpha=0.1)\n","rewards_init_0_eps_01_avg = rewards_init_0_eps_01 / n_runs"]},{"cell_type":"code","execution_count":null,"id":"95375ea9","metadata":{"id":"95375ea9"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.plot(freq_optimal_actions_init_5_eps_0, label=r'Optimistic $Q_1=5$ - Pure greed $\\epsilon$ = 0')\n","plt.plot(freq_optimal_actions_init_0_eps_01, label=r'Realistic $Q_1=0$ - Greedy $\\epsilon$ = 0.1')\n","plt.title(\"10-armed bandits - average over 2000 runs\\n\" + r\"$\\alpha=0.1$ fixed in update rule\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Frequency of choosing the optimal action\")\n","plt.legend()"]},{"cell_type":"markdown","id":"03157b60","metadata":{"id":"03157b60"},"source":["## UCB"]},{"cell_type":"code","execution_count":null,"id":"3f15bbd8","metadata":{"id":"3f15bbd8"},"outputs":[],"source":["rewards_ucb, freq_optimal_actions_ucb = simulation(n_machines, n_tokens, n_runs, algorithm='ucb', alpha=0.1)\n","rewards_ucb_avg = rewards_ucb / n_runs"]},{"cell_type":"code","execution_count":null,"id":"50f5a6c3","metadata":{"id":"50f5a6c3"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.plot(freq_optimal_actions_ucb, label=r'UCB c=2')\n","plt.plot(freq_optimal_actions_init_0_eps_01, label=r'Realistic $Q_1=0$ - Greedy $\\epsilon$ = 0.1')\n","plt.title(\"10-armed bandits - average over 2000 runs\\n\" + r\"$\\alpha=0.1$ fixed in update rule\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Frequency of choosing the optimal action\")\n","plt.legend()"]},{"cell_type":"code","source":["# Variable\n","n_iter = 1000 # iterations\n","k = 10 # number of machines\n","esperance_vector = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) # Q* for each machine\n","number_machine = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","preferance_vector = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]) # Initial weight for each machine\n","learning_rate = 0.5 # alpha\n","\n","# Functions\n","\n","\n","def reward(machine_x):\n","    result = np.random.normal(machine_x, 1) # give the result from the machine i\n","    return result"],"metadata":{"id":"iFaz-kaE8Ce1"},"id":"iFaz-kaE8Ce1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"634f7e4e","metadata":{"id":"634f7e4e"},"outputs":[],"source":["def GradientBandit(bandits, iterations, step_size, no_baseline=False):\n","    k = len(bandits)\n","    H = np.zeros(k)\n","    P = np.ones(k) / k\n","    N = np.zeros(k)\n","\n","    is_optimal = np.zeros(iterations)\n","    avg_reward = 0\n","    for t in range(1, iterations + 1): \n","        A = np.random.choice(number_machine, p=preference_vector)\n","        R = reward(esperance_vector[A])\n","        avg_reward = avg_reward + (1.0 / t) * (R - avg_reward)\n","        baseline = 0.0 if no_baseline else avg_reward\n","\n","        H[A] = H[A] + step_size * (R - baseline) * (1 - P[A])\n","        for a in range(k):\n","            if a != A:\n","                H[a] = H[a] - step_size * (R - baseline) * P[a]\n","\n","\n","        aux_exp = np.exp(H)\n","        P = aux_exp / np.sum(aux_exp)\n","        is_optimal[t - 1] = int(bandits.is_optimal(A)) * 1.0\n","\n","\n","    return P, is_optimal"]},{"cell_type":"code","execution_count":null,"id":"21674aec","metadata":{"id":"21674aec"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"29dfd561","metadata":{"id":"29dfd561"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3810jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"K-Bandits TP1.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}