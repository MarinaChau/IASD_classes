{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectOML_MarinaCHAU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNunYnBKlHRkM8+41T3fspV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaChau/IASD_classes/blob/master/OML/ProjectOML_MarinaCHAU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Second-order optimization methods"
      ],
      "metadata": {
        "id": "UfLnjKB3CSsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preamble: useful toolboxes, librairies, functions, etc.\n",
        "\n",
        "# Display\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import sqrt # Square root\n",
        "\n",
        "# NumPy - Matrix and vector structures\n",
        "import numpy as np # NumPy library\n",
        "from numpy.random import multivariate_normal, randn, uniform # Probability distributions\n",
        "\n",
        "# SciPy - Efficient mathematical calculation\n",
        "from scipy.linalg import norm # Euclidean norm\n",
        "from scipy.linalg.special_matrices import toeplitz # Toeplitz matrices\n",
        "from scipy.linalg import svdvals # Singular value decomposition\n",
        "from scipy.optimize import check_grad # Check derivatives\n",
        "from scipy.optimize import fmin_l_bfgs_b # Efficient method for minimization"
      ],
      "metadata": {
        "id": "rnLuKCgxCjNB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Newton's method"
      ],
      "metadata": {
        "id": "3s8M_bawNdFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation 1.1"
      ],
      "metadata": {
        "id": "g2zEGYK5NkCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newton_method(w0, function_f, gradient_f, hessian_f, n_iter, eps = 0.0001):\n",
        "    \"\"\"\n",
        "        A code for  Newton's method in its basic form\n",
        "\n",
        "        Inputs:\n",
        "            w0: Initial vector\n",
        "            function_f: Objective function\n",
        "            gradient_f: Gradient of the objective function\n",
        "            hessian_f: hessian matrix of the objective function\n",
        "            n_iter: Number of iterations\n",
        "            eps: convergence stop\n",
        "\n",
        "        Outputs:\n",
        "            w_output: Final iterate of the method\n",
        "    \"\"\"\n",
        "\n",
        "        ############\n",
        "    # Initialize iteration counter\n",
        "    k=1\n",
        "    ####################\n",
        "    # Main loop\n",
        "    w = w0 - np.dot(np.linalg.inv(hessian_f(w0)), gradient_f(w0))\n",
        "    while (np.linalg.norm(grad_f(w), 2) > eps and k < n_iter):\n",
        "        \n",
        "        # Select the stepsize and perform the update\n",
        "        w = w - np.dot(np.linalg.inv(hessian_f(w)), gradient_f(w))\n",
        "        \n",
        "        # Increment the iteration count\n",
        "        k += 1\n",
        "        \n",
        "    \n",
        "    # End main loop\n",
        "    ######################\n",
        "    # Output\n",
        "    print(w)\n",
        "    w_output = w.copy()\n",
        "    return w_output, k\n",
        "\n",
        "def newton(w_0, grad_f, hess_f, n_max = 1000, eps = 0.0001):#Here f is useless\n",
        "    n = 1\n",
        "    w_k = w_0 - np.dot(np.linalg.inv(hess_f(w_0)), grad_f(w_0))\n",
        "    while np.linalg.norm(grad_f(w_k), 2) > eps and n < n_max:\n",
        "        w_k = w_k - np.dot(np.linalg.inv(hess_f(w_k)), grad_f(w_k))\n",
        "        n+=1\n",
        "    return w_k, n"
      ],
      "metadata": {
        "id": "hWIZofCKDVHh"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1.1 \n",
        "\n",
        "a) \n",
        "\n",
        "minimize  $  w∈R^3 q(w) := 2(w1 + w2 + w3 − 3)^2 + (w1 − w2)^2 + (w2 − w3)^2 $\n",
        "\n",
        "We have: $\\nabla f (w) = \n",
        "\\begin{bmatrix}\n",
        "          6w_1 + 2w_2 + 4w_3 -12 \\\\\n",
        "          2w_1 + 8w_2 + 2w_3 -12\\\\     \n",
        "          4w_1 + 2w_2 + 6w_3 -12\n",
        "\\end{bmatrix}$\n",
        "and the hessian $\\nabla^2f(w) = \\begin{bmatrix}\n",
        "          6 & 2 & 4\\\\\n",
        "          2 & 8 & 2\\\\\n",
        "          4 & 2 & 6\n",
        "\\end{bmatrix}$\n",
        "\n",
        "After inverting the hessian:\n",
        "$\\nabla^2f(w)^{-1} = \\begin{bmatrix}\n",
        "          \\frac{11}{36} & -\\frac{1}{36} & -\\frac{7}{36}\\\\\n",
        "          -\\frac{1}{36} & \\frac{5}{36} & -\\frac{1}{36}\\\\\n",
        "          -\\frac{7}{36} & -\\frac{1}{36} & \\frac{11}{36}\n",
        "\\end{bmatrix}$\n",
        "\n",
        "So at step 1: with $ w_0 = \\begin{bmatrix}\n",
        "          0\\\\\n",
        "          0\\\\\n",
        "          0\n",
        "\\end{bmatrix}$\n",
        "\n",
        "We get,\n",
        "\n",
        "$w_1 = w_0 − [\\nabla^2 f (w_0)]^{-1} \\nabla f (w_0) = -\\begin{bmatrix}\n",
        "          \\frac{11}{36} & -\\frac{1}{36} & -\\frac{7}{36}\\\\\n",
        "          -\\frac{1}{36} & \\frac{5}{36} & -\\frac{1}{36}\\\\\n",
        "          -\\frac{7}{36} & -\\frac{1}{36} & \\frac{11}{36}\n",
        "\\end{bmatrix}  \\begin{bmatrix}\n",
        "           -12 \\\\\n",
        "           -12\\\\     \n",
        "           -12\n",
        "\\end{bmatrix}$ =$\\begin{bmatrix}\n",
        "           1 \\\\\n",
        "           1 \\\\     \n",
        "           1\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "Thus, after one iteration, it converges to the solution with Newton's method. \n",
        "\n"
      ],
      "metadata": {
        "id": "_tj8qK-WaY9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b)"
      ],
      "metadata": {
        "id": "R3OU_aMYj_4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute objective function f\n",
        "def function(w_k):\n",
        "    return 2*(w_k[0] + w_k[1] + w_k[2] -3)**2 + (w_k[0] - w_k[1])**2 + (w_k[1]- w_k[2])**2\n",
        "\n",
        "\n",
        "# Compute gradient_f\n",
        "def grad_f(w_k):\n",
        "    gradient_f = np.array([6* w_k[0] + 2 * w_k[1] + 4 * w_k[2] - 12,\n",
        "                           2 * w_k[0] + 8 * w_k[1] + 2 * w_k[2] -12,\n",
        "                           4 * w_k[0] + 2 * w_k[1] + 6 * w_k[2] - 12])\n",
        "    return gradient_f\n",
        "\n",
        "# Compute hessian_f\n",
        "def hess_f(w):\n",
        "    return np.array(([6,2,4],[2,8,2], [4, 2,6]))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "77_JSfwzkFrt"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different starting points\n",
        "w_origin = np.array([0, 0, 0])\n",
        "w_1 = np.array([1, 1, 1])\n",
        "w_2 = np.array([1, 0, 0])\n",
        "n_iter= 1000"
      ],
      "metadata": {
        "id": "KtT8CH3-jvKV"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_origin_star, n_iter_origin = newton_method(w_origin, function, grad_f, hess_f, n_iter)\n",
        "print(f\"With starting point {w_origin} it has converged in {n_iter_origin} steps.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFhZJvGTlfYH",
        "outputId": "9e3d22ce-6254-4569-a930-cef9dedc2e04"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1.]\n",
            "With starting point [0 0 0] it has converged in 1 steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_1_star, n_iter_1 = newton_method(w_1,  function, grad_f, hess_f, n_iter)\n",
        "print(f\"With starting point {w_1} it has converged in {n_iter_1} steps.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiV1HUA4l4yR",
        "outputId": "8d42f03a-a3e0-42ec-bd9c-d40623d4a823"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1.]\n",
            "With starting point [1 1 1] it has converged in 1 steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_2_star, n_iter_2 = newton_method(w_2,  function, grad_f, hess_f, n_iter)\n",
        "print(f\"With starting point {w_2} it has converged in {n_iter_2} steps.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTLcRzVTm3H4",
        "outputId": "fa4463cd-61bc-4627-e72f-8c1e5cabe729"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1.]\n",
            "With starting point [1 0 0] it has converged in 1 steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe convergence in one iteration for other starting points.\n",
        "\n"
      ],
      "metadata": {
        "id": "T9HtV2_hm7JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1.2"
      ],
      "metadata": {
        "id": "P6HiPUbxqGg8"
      }
    }
  ]
}