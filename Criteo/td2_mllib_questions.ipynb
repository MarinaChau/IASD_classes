{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaChau/IASD_classes/blob/master/Criteo/td2_mllib_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presentation\n",
        "In this workshop we will discover Mllib features, and apply them on the titanic dataset.\n",
        "\n",
        "We will try to predict passenger survival rate based on a few features, with a logistic regression model."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "51cf46e8-e114-407b-bf24-fef10f9aea04"
        },
        "id": "NQ4qqagJcYJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark Environment\n",
        "Since we are not running on databricks, we will need to install Spark by ourselves, every time we run the session.  \n",
        "We need to install Spark, as well as a Java Runtime Environment.  \n",
        "Then we need to setup a few environment variables."
      ],
      "metadata": {
        "id": "xKQyyLdvcoEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "KBn3abNFcrtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f817bbb-9eb4-47a5-b68c-2e60f7969bd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  287M  100  287M    0     0   231M      0  0:00:01  0:00:01 --:--:--  231M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "NKuD6v4JcztV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "1KjXADSOc4FW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional step : Enable SparkUI through secure tunnel\n",
        "This step is useful if you want to look at Spark UI.\n",
        "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
        "Then connect on the website and copy your AuthToken."
      ],
      "metadata": {
        "id": "vdav5Skhc9K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step downloads ngrok, configures your AuthToken, then starts the tunnel\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok authtoken my_ngrok_auth_token_retrieved_from_website # <-------------- change this line !\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "metadata": {
        "id": "keAsFF7BdAo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1accb3a1-9647-475d-de23-ae20e985caa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-01 12:49:47--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.161.241.46, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  3.35MB/s    in 4.9s    \n",
            "\n",
            "2022-04-01 12:49:52 (2.67 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now** get the Spark UI url on https://dashboard.ngrok.com/endpoints/status. We're done !"
      ],
      "metadata": {
        "id": "r_5OMsdudEw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset\n",
        "We need to download dataset and put it inside HDFS."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56b3941f-b4d2-465e-b2a9-edd7eb069b66"
        },
        "id": "8HjsFEkXcYJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset, make sure it is available on your gateway\n",
        "import urllib\n",
        "import urllib.request\n",
        "def get_dbutils(spark):\n",
        "        try:\n",
        "            from pyspark.dbutils import DBUtils\n",
        "            dbutils = DBUtils(spark)\n",
        "        except ImportError:\n",
        "            import IPython\n",
        "            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n",
        "        return dbutils\n",
        "\n",
        "dbutils = get_dbutils(spark)\n",
        "\n",
        "url = \"https://www.dropbox.com/s/1tl236ptjuwvcib/titanic-passengers.csv?dl=1\"\n",
        "urllib.request.urlretrieve(url, \"titanic.csv\")\n",
        "dbutils.fs.ls(\"file:/databricks/driver/\")\n",
        "\n",
        "# move the dataset to the file storage\n",
        "dbutils.fs.mv(\"file:/databricks/driver/titanic.csv\", \"dbfs:/titanic.csv\", recurse=True)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "117f3fe7-056b-477c-8761-43c47a4680e6"
        },
        "id": "x4TpeiwBcYJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "83729393-5818-476c-f268-185df98af655"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-27cc669c3524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.dropbox.com/s/1tl236ptjuwvcib/titanic-passengers.csv?dl=1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"titanic.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:/databricks/driver/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# move the dataset to the file storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset, make sure it is available on your gateway\n",
        "from urllib import request\n",
        "url = \"https://www.dropbox.com/s/1tl236ptjuwvcib/titanic-passengers.csv?dl=1\"\n",
        "\n",
        "request.urlretrieve(url, \"titanic.csv\")"
      ],
      "metadata": {
        "id": "u5OhHqs_LrgN",
        "outputId": "9f3bb713-ff4c-41a7-8118-50364e631e46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('titanic.csv', <http.client.HTTPMessage at 0x7f14bee15490>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools of the trade\n",
        "We need a few imports to learn some model with MLLib."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2bc1d6d0-4ec8-44ef-bc8d-7b840e4a0d89"
        },
        "id": "ZaWhfZwocYJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F # you already know this one ! need it whenever you want to transform columns\n",
        "from pyspark.ml.feature import *       # this package contains most of mllib feature engineering tools\n",
        "from pyspark.ml import Pipeline        # pipeline is used to combine features"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55e21102-1244-482c-afcb-5aabc90c3dc3"
        },
        "id": "gqlmf09kcYJQ"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 0\n",
        "Load the dataset.\n",
        "\n",
        "Make sure the remainder of the schema is correct."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ded2ba2e-2331-4d83-9046-cef307969cd3"
        },
        "id": "4EWsDxi0cYJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_2_df = spark.read.csv(\"titanic.csv\", sep=\";\", header=True, inferSchema=True)\n",
        "csv_2_df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b13970e3-3b8c-4f32-929d-fb11f0caf844"
        },
        "id": "b5yOFDlDcYJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23384b49-2f7d-4708-fa05-fd8e208d7a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+-----------------+------------------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|           Ticket|              Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+-----------------+------------------+-----+--------+\n",
            "|        343|      No|     2|Collander, Mr. Er...|  male|28.0|    0|    0|           248740|              13.0| null|       S|\n",
            "|         76|      No|     3|Moen, Mr. Sigurd ...|  male|25.0|    0|    0|           348123|              7.65|F G73|       S|\n",
            "|        641|      No|     3|Jensen, Mr. Hans ...|  male|20.0|    0|    0|           350050|7.8542000000000005| null|       S|\n",
            "|        568|      No|     3|Palsson, Mrs. Nil...|female|29.0|    0|    4|           349909|            21.075| null|       S|\n",
            "|        672|      No|     1|Davidson, Mr. Tho...|  male|31.0|    1|    0|       F.C. 12750|              52.0|  B71|       S|\n",
            "|        105|      No|     3|Gustafsson, Mr. A...|  male|37.0|    2|    0|          3101276|             7.925| null|       S|\n",
            "|        576|      No|     3|Patchett, Mr. George|  male|19.0|    0|    0|           358585|              14.5| null|       S|\n",
            "|        382|     Yes|     3|\"Nakid, Miss. Mar...|female| 1.0|    0|    2|             2653|           15.7417| null|       C|\n",
            "|        228|      No|     3|\"Lovell, Mr. John...|  male|20.5|    0|    0|        A/5 21173|              7.25| null|       S|\n",
            "|        433|     Yes|     2|Louch, Mrs. Charl...|female|42.0|    1|    0|       SC/AH 3085|              26.0| null|       S|\n",
            "|        135|      No|     2|Sobey, Mr. Samuel...|  male|25.0|    0|    0|       C.A. 29178|              13.0| null|       S|\n",
            "|        294|      No|     3| Haas, Miss. Aloisia|female|24.0|    0|    0|           349236|              8.85| null|       S|\n",
            "|        755|     Yes|     2|Herman, Mrs. Samu...|female|48.0|    1|    2|           220845|              65.0| null|       S|\n",
            "|        595|      No|     2|Chapman, Mr. John...|  male|37.0|    1|    0|      SC/AH 29037|              26.0| null|       S|\n",
            "|        127|      No|     3| McMahon, Mr. Martin|  male|null|    0|    0|           370372|              7.75| null|       Q|\n",
            "|        434|      No|     3|Kallio, Mr. Nikol...|  male|17.0|    0|    0|STON/O 2. 3101274|             7.125| null|       S|\n",
            "|        378|      No|     1|Widener, Mr. Harr...|  male|27.0|    0|    2|           113503|             211.5|  C82|       C|\n",
            "|        533|      No|     3|Elias, Mr. Joseph Jr|  male|17.0|    1|    1|             2690|            7.2292| null|       C|\n",
            "|        666|      No|     2|  Hickman, Mr. Lewis|  male|32.0|    2|    0|     S.O.C. 14879|              73.5| null|       S|\n",
            "|        225|     Yes|     1|Hoyt, Mr. Frederi...|  male|38.0|    1|    0|            19943|              90.0|  C93|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+-----------------+------------------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "csv_2_df.printSchema()"
      ],
      "metadata": {
        "id": "oZzmZUtSML2G",
        "outputId": "0dd18d59-71ad-46fc-dfb6-9d3209d86361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: string (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = csv_2_df.cache().randomSplit([0.9, 0.1], seed=12345)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9fbd51b0-33aa-4331-a26d-e7c92aaeb6de"
        },
        "id": "C8ce3yXUcYJR"
      },
      "outputs": [],
      "execution_count": 44
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "On training set, fit a model that predicts passenger survival probability, function of ticket price.\n",
        "\n",
        "You will need to convert survived column in 0/1 to pass it to the logistic regression. Transform it with StringIndexer.\n",
        "\n",
        "Use a pipeline ending with a logistic regression.\n",
        "\n",
        "Compute model AUC on validation set.\n",
        "\n",
        "Documentation:\n",
        "- https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n",
        "- https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline\n",
        "- https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#binary-classification"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "463001f6-503d-4ab9-8654-f31637582886"
        },
        "id": "sdxyDP9xcYJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "06ce7c3d-3d2e-4bde-9560-ed9171bf58a0"
        },
        "id": "QDpTm8CYcYJT"
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Survived column with StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_index\")\n",
        "indexed = indexer.fit(train).transform(train)\n",
        "indexed.show()"
      ],
      "metadata": {
        "id": "eIM9xq9bNato",
        "outputId": "86676bd6-b75b-4a17-b439-c19a770ac298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|Survived_index|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------+\n",
            "|          1|      No|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|              7.25| null|       S|           0.0|\n",
            "|          2|     Yes|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|           71.2833|  C85|       C|           1.0|\n",
            "|          3|     Yes|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|             7.925| null|       S|           1.0|\n",
            "|          4|     Yes|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|              53.1| C123|       S|           1.0|\n",
            "|          5|      No|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|              8.05| null|       S|           0.0|\n",
            "|          6|      No|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|            8.4583| null|       Q|           0.0|\n",
            "|          7|      No|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|           51.8625|  E46|       S|           0.0|\n",
            "|          8|      No|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|            21.075| null|       S|           0.0|\n",
            "|          9|     Yes|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|           11.1333| null|       S|           1.0|\n",
            "|         10|     Yes|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|           30.0708| null|       C|           1.0|\n",
            "|         11|     Yes|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|              16.7|   G6|       S|           1.0|\n",
            "|         12|     Yes|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|             26.55| C103|       S|           1.0|\n",
            "|         13|      No|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|              8.05| null|       S|           0.0|\n",
            "|         14|      No|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|            31.275| null|       S|           0.0|\n",
            "|         15|      No|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|7.8542000000000005| null|       S|           0.0|\n",
            "|         16|     Yes|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|              16.0| null|       S|           1.0|\n",
            "|         17|      No|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|            29.125| null|       Q|           0.0|\n",
            "|         18|     Yes|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|              13.0| null|       S|           1.0|\n",
            "|         19|      No|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|              18.0| null|       S|           0.0|\n",
            "|         20|     Yes|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|             7.225| null|       C|           1.0|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer"
      ],
      "metadata": {
        "id": "FuSMVNUwRx2B"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed['Survived_index', 'Fare']"
      ],
      "metadata": {
        "id": "2kHmdfHKTYdB",
        "outputId": "1ea0e7c9-6210-4fd0-de41-172222ed80e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Survived_index: double, Fare: double]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"Fare\"],outputCol=\"Fare_vec\")\n",
        "indexed = assembler.transform(indexed)\n",
        "indexed.show(5)\n"
      ],
      "metadata": {
        "id": "n2Jj5ohJVUJn",
        "outputId": "44feccd6-70b3-4a5d-8365-87d89cda107e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+---------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Survived_index| Fare_vec|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+---------+\n",
            "|          1|      No|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|           0.0|   [7.25]|\n",
            "|          2|     Yes|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|           1.0|[71.2833]|\n",
            "|          3|     Yes|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|           1.0|  [7.925]|\n",
            "|          4|     Yes|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|           1.0|   [53.1]|\n",
            "|          5|      No|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|           0.0|   [8.05]|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure an ML pipeline\n",
        "indexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_index\")\n",
        "assembler = VectorAssembler(inputCols=[\"Fare\"],outputCol=\"Fare_vec\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001, featuresCol='Fare_vec', labelCol='Survived_index')\n",
        "pipeline = Pipeline(stages=[indexer, assembler, lr])\n",
        "\n",
        "training = train\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "model = pipeline.fit(training)\n",
        "\n",
        "# Make predictions on training documents and print columns of interest.\n",
        "prediction = model.transform(test)"
      ],
      "metadata": {
        "id": "zzXykPAfRzZG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = prediction.select(\"Survived_index\", \"rawPrediction\")\n",
        "preds = preds.withColumnRenamed(\"Survived_index\",\"label\")"
      ],
      "metadata": {
        "id": "vyZg-ObQWf32"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "\n",
        "evaluation = evaluator.evaluate(preds)\n",
        "\n",
        "print('Test Area Under Roc',evaluation)"
      ],
      "metadata": {
        "id": "JsvxHEgKYHwn",
        "outputId": "5d707395-8822-4821-a783-0af83ad3f8f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under Roc 0.7175925925925926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "We will do a lots of feature engineering now and we don't want you to copy-paste code all-way long.\n",
        "\n",
        "Write the following function:\n",
        "\n",
        "Inputs:\n",
        "- pipeline\n",
        "- training set\n",
        "- validation set\n",
        "\n",
        "Outputs:\n",
        "- auc\n",
        "- transformed dataset (with prediction)\n",
        "\n",
        "Make sure it returns on previous pipeline."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4579a58c-9e9b-4f9a-bd32-5feecd3dd41d"
        },
        "id": "hIkoTpzqcYJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f11a5d25-2e33-4200-b447-e987b7e08878"
        },
        "id": "OQ4tkBJKcYJV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "26583cd3-baea-4ec8-81ca-d4fca488d933"
        },
        "id": "J_582kkYcYJV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n",
        "Relying on raw continuous feature may be a bit rough.\n",
        "We can try to bucketize numeric feature in five buckets instead."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b03803ba-1430-4794-8f51-f4db5801fdbb"
        },
        "id": "W8u0vwkwcYJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "87482d50-c46e-45e7-84e5-44bbbce657af"
        },
        "id": "bo6Um4pqcYJX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "Why don't you try to rely on other numerical features now ?\n",
        "\n",
        "You can try to leverage 'Age', and maybe 'PassengerId' while we're at it.\n",
        "\n",
        "Is it better ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a83bf019-a813-4107-8c4a-493ca86046a1"
        },
        "id": "EfQpW9vrcYJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b390460e-373a-413e-afe1-7544bf4ef83c"
        },
        "id": "a8OWIgsecYJX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5\n",
        "We should try to use categorial features.\n",
        "\n",
        "Remember, spark just understands vectors. So you need to convert categories in vectors with OneHotEncoder.\n",
        "\n",
        "Try several categories and identify what works."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f3cd8a2b-ab7b-4ab9-a1cf-29b7ecde5f79"
        },
        "id": "PgfrY5wOcYJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "27703893-d86a-4127-b6d9-afbf306bc1d4"
        },
        "id": "epzNBVHrcYJY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sex is not numeric, we need to convert it before one-hot-encoding it !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c8633f54-965a-463d-94c3-a7bdfee5081a"
        },
        "id": "N-QwYY0bcYJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "521f533a-9c49-4cee-ba30-d880c13306a7"
        },
        "id": "HQCbgWOscYJZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6\n",
        "\n",
        "These are open questions you can try to tackle in any order:\n",
        "- cross features. E.g., try to use features like : passenger is male and passenger is older than 30 years.\n",
        "- use feature hashing\n",
        "- rely on name feature"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1427b306-351d-42b7-afa2-7f54664141f8"
        },
        "id": "rmrdVdf5cYJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Hashing\n",
        "In this one, you will need to create a custom transformation that transforms a sparse vector into another sparse vector with lower dimension (MLLib does not have exactly what we want there).\n",
        "- you can rely on this post to see how to create transformer : https://csyhuang.github.io/2020/08/01/custom-transformer/\n",
        "- look at the following classes for your udf : VectorUDT ; SparseVector"
      ],
      "metadata": {
        "id": "w5ZxLJasShZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dmZOZZWjSgJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Titanic + MLLib (questions) (1)",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 323843935193526
    },
    "colab": {
      "name": "td2-mllib-questions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}