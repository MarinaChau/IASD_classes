{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaChau/IASD_classes/blob/master/Criteo/td2_mllib_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presentation\n",
        "In this workshop we will discover Mllib features, and apply them on the titanic dataset.\n",
        "\n",
        "We will try to predict passenger survival rate based on a few features, with a logistic regression model."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "51cf46e8-e114-407b-bf24-fef10f9aea04"
        },
        "id": "NQ4qqagJcYJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark Environment\n",
        "Since we are not running on databricks, we will need to install Spark by ourselves, every time we run the session.  \n",
        "We need to install Spark, as well as a Java Runtime Environment.  \n",
        "Then we need to setup a few environment variables."
      ],
      "metadata": {
        "id": "xKQyyLdvcoEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "KBn3abNFcrtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f817bbb-9eb4-47a5-b68c-2e60f7969bd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  287M  100  287M    0     0   231M      0  0:00:01  0:00:01 --:--:--  231M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "NKuD6v4JcztV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "1KjXADSOc4FW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional step : Enable SparkUI through secure tunnel\n",
        "This step is useful if you want to look at Spark UI.\n",
        "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
        "Then connect on the website and copy your AuthToken."
      ],
      "metadata": {
        "id": "vdav5Skhc9K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step downloads ngrok, configures your AuthToken, then starts the tunnel\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok authtoken my_ngrok_auth_token_retrieved_from_website # <-------------- change this line !\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "metadata": {
        "id": "keAsFF7BdAo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1accb3a1-9647-475d-de23-ae20e985caa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-01 12:49:47--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.161.241.46, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  3.35MB/s    in 4.9s    \n",
            "\n",
            "2022-04-01 12:49:52 (2.67 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now** get the Spark UI url on https://dashboard.ngrok.com/endpoints/status. We're done !"
      ],
      "metadata": {
        "id": "r_5OMsdudEw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset\n",
        "We need to download dataset and put it inside HDFS."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56b3941f-b4d2-465e-b2a9-edd7eb069b66"
        },
        "id": "8HjsFEkXcYJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset, make sure it is available on your gateway\n",
        "import urllib\n",
        "import urllib.request\n",
        "def get_dbutils(spark):\n",
        "        try:\n",
        "            from pyspark.dbutils import DBUtils\n",
        "            dbutils = DBUtils(spark)\n",
        "        except ImportError:\n",
        "            import IPython\n",
        "            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n",
        "        return dbutils\n",
        "\n",
        "dbutils = get_dbutils(spark)\n",
        "\n",
        "url = \"https://www.dropbox.com/s/1tl236ptjuwvcib/titanic-passengers.csv?dl=1\"\n",
        "urllib.request.urlretrieve(url, \"titanic.csv\")\n",
        "dbutils.fs.ls(\"file:/databricks/driver/\")\n",
        "\n",
        "# move the dataset to the file storage\n",
        "dbutils.fs.mv(\"file:/databricks/driver/titanic.csv\", \"dbfs:/titanic.csv\", recurse=True)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "117f3fe7-056b-477c-8761-43c47a4680e6"
        },
        "id": "x4TpeiwBcYJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "83729393-5818-476c-f268-185df98af655"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-27cc669c3524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.dropbox.com/s/1tl236ptjuwvcib/titanic-passengers.csv?dl=1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"titanic.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:/databricks/driver/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# move the dataset to the file storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset, make sure it is available on your gateway\n",
        "from urllib import request\n",
        "url = \"https://www.dropbox.com/s/1tl236ptjuwvcib/titanic-passengers.csv?dl=1\"\n",
        "\n",
        "request.urlretrieve(url, \"titanic.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5OhHqs_LrgN",
        "outputId": "9f3bb713-ff4c-41a7-8118-50364e631e46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('titanic.csv', <http.client.HTTPMessage at 0x7f14bee15490>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools of the trade\n",
        "We need a few imports to learn some model with MLLib."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2bc1d6d0-4ec8-44ef-bc8d-7b840e4a0d89"
        },
        "id": "ZaWhfZwocYJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F # you already know this one ! need it whenever you want to transform columns\n",
        "from pyspark.ml.feature import *       # this package contains most of mllib feature engineering tools\n",
        "from pyspark.ml import Pipeline        # pipeline is used to combine features"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55e21102-1244-482c-afcb-5aabc90c3dc3"
        },
        "id": "gqlmf09kcYJQ"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 0\n",
        "Load the dataset.\n",
        "\n",
        "Make sure the remainder of the schema is correct."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ded2ba2e-2331-4d83-9046-cef307969cd3"
        },
        "id": "4EWsDxi0cYJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_2_df = spark.read.csv(\"titanic.csv\", sep=\";\", header=True, inferSchema=True)\n",
        "csv_2_df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b13970e3-3b8c-4f32-929d-fb11f0caf844"
        },
        "id": "b5yOFDlDcYJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23384b49-2f7d-4708-fa05-fd8e208d7a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+-----------------+------------------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|           Ticket|              Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+-----------------+------------------+-----+--------+\n",
            "|        343|      No|     2|Collander, Mr. Er...|  male|28.0|    0|    0|           248740|              13.0| null|       S|\n",
            "|         76|      No|     3|Moen, Mr. Sigurd ...|  male|25.0|    0|    0|           348123|              7.65|F G73|       S|\n",
            "|        641|      No|     3|Jensen, Mr. Hans ...|  male|20.0|    0|    0|           350050|7.8542000000000005| null|       S|\n",
            "|        568|      No|     3|Palsson, Mrs. Nil...|female|29.0|    0|    4|           349909|            21.075| null|       S|\n",
            "|        672|      No|     1|Davidson, Mr. Tho...|  male|31.0|    1|    0|       F.C. 12750|              52.0|  B71|       S|\n",
            "|        105|      No|     3|Gustafsson, Mr. A...|  male|37.0|    2|    0|          3101276|             7.925| null|       S|\n",
            "|        576|      No|     3|Patchett, Mr. George|  male|19.0|    0|    0|           358585|              14.5| null|       S|\n",
            "|        382|     Yes|     3|\"Nakid, Miss. Mar...|female| 1.0|    0|    2|             2653|           15.7417| null|       C|\n",
            "|        228|      No|     3|\"Lovell, Mr. John...|  male|20.5|    0|    0|        A/5 21173|              7.25| null|       S|\n",
            "|        433|     Yes|     2|Louch, Mrs. Charl...|female|42.0|    1|    0|       SC/AH 3085|              26.0| null|       S|\n",
            "|        135|      No|     2|Sobey, Mr. Samuel...|  male|25.0|    0|    0|       C.A. 29178|              13.0| null|       S|\n",
            "|        294|      No|     3| Haas, Miss. Aloisia|female|24.0|    0|    0|           349236|              8.85| null|       S|\n",
            "|        755|     Yes|     2|Herman, Mrs. Samu...|female|48.0|    1|    2|           220845|              65.0| null|       S|\n",
            "|        595|      No|     2|Chapman, Mr. John...|  male|37.0|    1|    0|      SC/AH 29037|              26.0| null|       S|\n",
            "|        127|      No|     3| McMahon, Mr. Martin|  male|null|    0|    0|           370372|              7.75| null|       Q|\n",
            "|        434|      No|     3|Kallio, Mr. Nikol...|  male|17.0|    0|    0|STON/O 2. 3101274|             7.125| null|       S|\n",
            "|        378|      No|     1|Widener, Mr. Harr...|  male|27.0|    0|    2|           113503|             211.5|  C82|       C|\n",
            "|        533|      No|     3|Elias, Mr. Joseph Jr|  male|17.0|    1|    1|             2690|            7.2292| null|       C|\n",
            "|        666|      No|     2|  Hickman, Mr. Lewis|  male|32.0|    2|    0|     S.O.C. 14879|              73.5| null|       S|\n",
            "|        225|     Yes|     1|Hoyt, Mr. Frederi...|  male|38.0|    1|    0|            19943|              90.0|  C93|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+-----------------+------------------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "csv_2_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZzmZUtSML2G",
        "outputId": "0dd18d59-71ad-46fc-dfb6-9d3209d86361"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: string (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = csv_2_df.cache().randomSplit([0.9, 0.1], seed=12345)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9fbd51b0-33aa-4331-a26d-e7c92aaeb6de"
        },
        "id": "C8ce3yXUcYJR"
      },
      "outputs": [],
      "execution_count": 44
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "On training set, fit a model that predicts passenger survival probability, function of ticket price.\n",
        "\n",
        "You will need to convert survived column in 0/1 to pass it to the logistic regression. Transform it with StringIndexer.\n",
        "\n",
        "Use a pipeline ending with a logistic regression.\n",
        "\n",
        "Compute model AUC on validation set.\n",
        "\n",
        "Documentation:\n",
        "- https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n",
        "- https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline\n",
        "- https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#binary-classification"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "463001f6-503d-4ab9-8654-f31637582886"
        },
        "id": "sdxyDP9xcYJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "06ce7c3d-3d2e-4bde-9560-ed9171bf58a0"
        },
        "id": "QDpTm8CYcYJT"
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Survived column with StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_index\")\n",
        "indexed = indexer.fit(train).transform(train)\n",
        "indexed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIM9xq9bNato",
        "outputId": "86676bd6-b75b-4a17-b439-c19a770ac298"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|Survived_index|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------+\n",
            "|          1|      No|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|              7.25| null|       S|           0.0|\n",
            "|          2|     Yes|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|           71.2833|  C85|       C|           1.0|\n",
            "|          3|     Yes|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|             7.925| null|       S|           1.0|\n",
            "|          4|     Yes|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|              53.1| C123|       S|           1.0|\n",
            "|          5|      No|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|              8.05| null|       S|           0.0|\n",
            "|          6|      No|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|            8.4583| null|       Q|           0.0|\n",
            "|          7|      No|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|           51.8625|  E46|       S|           0.0|\n",
            "|          8|      No|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|            21.075| null|       S|           0.0|\n",
            "|          9|     Yes|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|           11.1333| null|       S|           1.0|\n",
            "|         10|     Yes|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|           30.0708| null|       C|           1.0|\n",
            "|         11|     Yes|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|              16.7|   G6|       S|           1.0|\n",
            "|         12|     Yes|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|             26.55| C103|       S|           1.0|\n",
            "|         13|      No|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|              8.05| null|       S|           0.0|\n",
            "|         14|      No|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|            31.275| null|       S|           0.0|\n",
            "|         15|      No|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|7.8542000000000005| null|       S|           0.0|\n",
            "|         16|     Yes|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|              16.0| null|       S|           1.0|\n",
            "|         17|      No|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|            29.125| null|       Q|           0.0|\n",
            "|         18|     Yes|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|              13.0| null|       S|           1.0|\n",
            "|         19|      No|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|              18.0| null|       S|           0.0|\n",
            "|         20|     Yes|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|             7.225| null|       C|           1.0|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer"
      ],
      "metadata": {
        "id": "FuSMVNUwRx2B"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed['Survived_index', 'Fare']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kHmdfHKTYdB",
        "outputId": "1ea0e7c9-6210-4fd0-de41-172222ed80e1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Survived_index: double, Fare: double]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"Fare\"],outputCol=\"Fare_vec\")\n",
        "indexed = assembler.transform(indexed)\n",
        "indexed.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2Jj5ohJVUJn",
        "outputId": "44feccd6-70b3-4a5d-8365-87d89cda107e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+---------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Survived_index| Fare_vec|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+---------+\n",
            "|          1|      No|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|           0.0|   [7.25]|\n",
            "|          2|     Yes|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|           1.0|[71.2833]|\n",
            "|          3|     Yes|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|           1.0|  [7.925]|\n",
            "|          4|     Yes|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|           1.0|   [53.1]|\n",
            "|          5|      No|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|           0.0|   [8.05]|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure an ML pipeline\n",
        "indexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_index\")\n",
        "assembler = VectorAssembler(inputCols=[\"Fare\"],outputCol=\"Fare_vec\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001, featuresCol='Fare_vec', labelCol='Survived_index')\n",
        "pipeline = Pipeline(stages=[indexer, assembler, lr])\n",
        "\n",
        "training = train\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "model = pipeline.fit(training)\n",
        "\n",
        "# Make predictions on training documents and print columns of interest.\n",
        "prediction = model.transform(test)"
      ],
      "metadata": {
        "id": "zzXykPAfRzZG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiuOfq3AcuZA",
        "outputId": "2dc634e3-36cb-4199-b673-2a7059c4105d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PipelineModel_5d9b7a81ebbc"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = prediction.select(\"Survived_index\", \"rawPrediction\")\n",
        "preds = preds.withColumnRenamed(\"Survived_index\",\"label\")"
      ],
      "metadata": {
        "id": "vyZg-ObQWf32"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "\n",
        "evaluation = evaluator.evaluate(preds)\n",
        "\n",
        "print('Test Area Under Roc',evaluation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsvxHEgKYHwn",
        "outputId": "5d707395-8822-4821-a783-0af83ad3f8f2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under Roc 0.7175925925925926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "We will do a lots of feature engineering now and we don't want you to copy-paste code all-way long.\n",
        "\n",
        "Write the following function:\n",
        "\n",
        "Inputs:\n",
        "- pipeline\n",
        "- training set\n",
        "- validation set\n",
        "\n",
        "Outputs:\n",
        "- auc\n",
        "- transformed dataset (with prediction)\n",
        "\n",
        "Make sure it returns on previous pipeline."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4579a58c-9e9b-4f9a-bd32-5feecd3dd41d"
        },
        "id": "hIkoTpzqcYJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(pipeline, training_set, validation_set):\n",
        "\n",
        "    print(\"AliceTLabest\")\n",
        "\n",
        "    # Fit the pipeline to training documents.\n",
        "    model = pipeline.fit(training_set)\n",
        "\n",
        "    # Make predictions on training documents and print columns of interest.\n",
        "    prediction = model.transform(validation_set)\n",
        "\n",
        "    # get the modified training dataset\n",
        "    training_transformed = model.transform(training) \n",
        "\n",
        "    # Evaluate validation set with AUC\n",
        "    evaluation = evaluator.evaluate(preds)\n",
        "    print('Test Area Under Roc',evaluation)\n",
        "\n",
        "    return evaluation, training_transformed\n",
        "\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f11a5d25-2e33-4200-b447-e987b7e08878"
        },
        "id": "OQ4tkBJKcYJV"
      },
      "outputs": [],
      "execution_count": 81
    },
    {
      "cell_type": "code",
      "source": [
        "auc, df_transformed = make_prediction(pipeline, train, test)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "26583cd3-baea-4ec8-81ca-d4fca488d933"
        },
        "id": "J_582kkYcYJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38909ee-becb-4d54-fc9b-8c967aa33c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AliceTLabest\n",
            "Test Area Under Roc 0.7175925925925926\n"
          ]
        }
      ],
      "execution_count": 82
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n",
        "Relying on raw continuous feature may be a bit rough.\n",
        "We can try to bucketize numeric feature in five buckets instead."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b03803ba-1430-4794-8f51-f4db5801fdbb"
        },
        "id": "W8u0vwkwcYJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find min of subjects column\n",
        "df_transformed.agg({'Fare': 'min'}).show()\n",
        "\n",
        "# find max of subjects column\n",
        "df_transformed.agg({'Fare': 'max'}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdYUrkJpfhR2",
        "outputId": "bbe84cad-9dbd-4591-cd2f-a39fe5230846"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|min(Fare)|\n",
            "+---------+\n",
            "|      0.0|\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|max(Fare)|\n",
            "+---------+\n",
            "| 512.3292|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "buckets = np.arange(0, 513, 102)\n",
        "buckets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM85EokIf7yY",
        "outputId": "da27aca3-6d95-4695-af63-8813ad6ad784"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 102, 204, 306, 408, 510])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 5 buckets according to the column \"Fare\"\n",
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "qd = QuantileDiscretizer(numBuckets=5, inputCol=\"Fare\", outputCol=\"Buckets_fare\", relativeError=0.001)\n",
        "bucketizer = qd.fit(train)\n",
        "train_bucketed = bucketizer.transform(train)\n",
        "train_bucketed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYyQ2mb0gnCv",
        "outputId": "4effb4dc-22a2-4625-cb80-f9a13f31384e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+------------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|Buckets_fare|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+------------+\n",
            "|          1|      No|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|              7.25| null|       S|         0.0|\n",
            "|          2|     Yes|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|           71.2833|  C85|       C|         4.0|\n",
            "|          3|     Yes|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|             7.925| null|       S|         1.0|\n",
            "|          4|     Yes|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|              53.1| C123|       S|         4.0|\n",
            "|          5|      No|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|              8.05| null|       S|         1.0|\n",
            "|          6|      No|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|            8.4583| null|       Q|         1.0|\n",
            "|          7|      No|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|           51.8625|  E46|       S|         4.0|\n",
            "|          8|      No|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|            21.075| null|       S|         2.0|\n",
            "|          9|     Yes|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|           11.1333| null|       S|         2.0|\n",
            "|         10|     Yes|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|           30.0708| null|       C|         3.0|\n",
            "|         11|     Yes|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|              16.7|   G6|       S|         2.0|\n",
            "|         12|     Yes|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|             26.55| C103|       S|         3.0|\n",
            "|         13|      No|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|              8.05| null|       S|         1.0|\n",
            "|         14|      No|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|            31.275| null|       S|         3.0|\n",
            "|         15|      No|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|7.8542000000000005| null|       S|         0.0|\n",
            "|         16|     Yes|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|              16.0| null|       S|         2.0|\n",
            "|         17|      No|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|            29.125| null|       Q|         3.0|\n",
            "|         18|     Yes|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|              13.0| null|       S|         2.0|\n",
            "|         19|      No|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|              18.0| null|       S|         2.0|\n",
            "|         20|     Yes|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|             7.225| null|       C|         0.0|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "Why don't you try to rely on other numerical features now ?\n",
        "\n",
        "You can try to leverage 'Age', and maybe 'PassengerId' while we're at it.\n",
        "\n",
        "Is it better ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a83bf019-a813-4107-8c4a-493ca86046a1"
        },
        "id": "EfQpW9vrcYJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.filter(col(\"PassengerID\").isNull()).count()"
      ],
      "metadata": {
        "id": "cQp96d9zlGHK",
        "outputId": "19502804-bb99-4f9a-a377-a4ab7f467d12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "160"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.printSchema()"
      ],
      "metadata": {
        "id": "IN2EPAoyj1aW",
        "outputId": "d5f603c8-d6aa-4e0b-8e29-2f868c3e1731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: string (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "print(train.filter(col(\"Age\").isNotNull()).count())"
      ],
      "metadata": {
        "id": "sEn-yge0kjPy",
        "outputId": "bff716cd-d571-4d24-926c-512ddb98b884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure an ML pipeline\n",
        "indexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_index\")\n",
        "qd = QuantileDiscretizer(numBuckets=5, inputCol=\"Fare\", outputCol=\"Buckets_fare\", relativeError=0.001)\n",
        "assembler = VectorAssembler(inputCols=[\"Buckets_fare\", \"Age\"], outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001, featuresCol='features', labelCol='Survived_index')\n",
        "pipeline_2 = Pipeline(stages=[indexer, qd, assembler, lr])"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b390460e-373a-413e-afe1-7544bf4ef83c"
        },
        "id": "a8OWIgsecYJX"
      },
      "outputs": [],
      "execution_count": 117
    },
    {
      "cell_type": "code",
      "source": [
        "auc, df_transformed = make_prediction(pipeline_2, train, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZjZtpa36jLm3",
        "outputId": "ddf6d957-294a-4e63-c64d-fd52983c66f3"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AliceTLabest\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-54ce8bf6d339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-757588b54a04>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(pipeline, training_set, validation_set)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Fit the pipeline to training documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Make predictions on training documents and print columns of interest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2787.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 166.0 failed 1 times, most recent failure: Lost task 0.0 in stage 166.0 (TID 131) (703bda3ff209 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3516/854815904: (struct<Buckets_fare:double,Age:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 30 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:511)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3516/854815904: (struct<Buckets_fare:double,Age:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 30 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5\n",
        "We should try to use categorial features.\n",
        "\n",
        "Remember, spark just understands vectors. So you need to convert categories in vectors with OneHotEncoder.\n",
        "\n",
        "Try several categories and identify what works."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f3cd8a2b-ab7b-4ab9-a1cf-29b7ecde5f79"
        },
        "id": "PgfrY5wOcYJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "27703893-d86a-4127-b6d9-afbf306bc1d4"
        },
        "id": "epzNBVHrcYJY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sex is not numeric, we need to convert it before one-hot-encoding it !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c8633f54-965a-463d-94c3-a7bdfee5081a"
        },
        "id": "N-QwYY0bcYJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "521f533a-9c49-4cee-ba30-d880c13306a7"
        },
        "id": "HQCbgWOscYJZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6\n",
        "\n",
        "These are open questions you can try to tackle in any order:\n",
        "- cross features. E.g., try to use features like : passenger is male and passenger is older than 30 years.\n",
        "- use feature hashing\n",
        "- rely on name feature"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1427b306-351d-42b7-afa2-7f54664141f8"
        },
        "id": "rmrdVdf5cYJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Hashing\n",
        "In this one, you will need to create a custom transformation that transforms a sparse vector into another sparse vector with lower dimension (MLLib does not have exactly what we want there).\n",
        "- you can rely on this post to see how to create transformer : https://csyhuang.github.io/2020/08/01/custom-transformer/\n",
        "- look at the following classes for your udf : VectorUDT ; SparseVector"
      ],
      "metadata": {
        "id": "w5ZxLJasShZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dmZOZZWjSgJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Titanic + MLLib (questions) (1)",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 323843935193526
    },
    "colab": {
      "name": "td2-mllib-questions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}